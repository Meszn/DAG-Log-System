import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib
import os

# Ayarlar
DATA_FILE = "server_metrics.csv"
MODEL_PATH = "app/services/dag_model.pkl"
SCALER_PATH = "app/services/dag_scaler.pkl"


def generate_realistic_data(n_samples=5000):
    """
    GerÃ§ek bir sunucunun 1 haftalÄ±k CPU/RAM davranÄ±ÅŸÄ±nÄ± simÃ¼le eder.
    """
    print("ðŸ“Š GerÃ§ekÃ§i veri seti oluÅŸturuluyor...")

    # Zaman serisi (dakika dakika)
    time_index = np.arange(n_samples)

    # 1. GÃœNLÃœK DÃ–NGÃœ (Day/Night Cycle): Sunucular gÃ¼ndÃ¼z yoÄŸun, gece sakindir.
    # SinÃ¼s dalgasÄ± kullanarak gÃ¼nlÃ¼k yÃ¼kÃ¼ simÃ¼le ediyoruz.
    daily_cycle = np.sin(time_index * 2 * np.pi / (24 * 60))

    # 2. CPU OLUÅžTURMA
    # Baz YÃ¼k (%20) + GÃ¼nlÃ¼k DÃ¶ngÃ¼ Etkisi (%30) + Rastgele GÃ¼rÃ¼ltÃ¼ (%10)
    cpu_usage = 20 + (daily_cycle * 15) + np.random.normal(0, 5, n_samples)
    cpu_usage = np.clip(cpu_usage, 5, 100)  # 0-100 arasÄ±na sabitle

    # 3. RAM OLUÅžTURMA
    # Baz YÃ¼k (%40) + CPU ile hafif korelasyon + Rastgele GÃ¼rÃ¼ltÃ¼
    memory_usage = 40 + (cpu_usage * 0.5) + np.random.normal(0, 2, n_samples)
    memory_usage = np.clip(memory_usage, 10, 100)

    # DataFrame oluÅŸtur
    df = pd.DataFrame({
        'cpu_usage': cpu_usage,
        'memory_usage': memory_usage
    })

    # 4. ANOMALÄ° ENJEKSÄ°YONU (GerÃ§ek SaldÄ±rÄ±lar)
    # Sisteme %2 oranÄ±nda anormallik (aÅŸÄ±rÄ± yÃ¼k veya Ã§Ã¶kme) ekleyelim
    n_anomalies = int(n_samples * 0.02)
    indices = np.random.choice(n_samples, n_anomalies, replace=False)

    for i in indices:
        scenario = np.random.choice(['spike', 'leak', 'crash'])

        if scenario == 'spike':  # Ani CPU PatlamasÄ± (DDoS vb.)
            df.loc[i, 'cpu_usage'] = np.random.uniform(90, 100)
            df.loc[i, 'memory_usage'] = np.random.uniform(50, 80)

        elif scenario == 'leak':  # Memory Leak (RAM ÅŸiÅŸmesi)
            df.loc[i, 'cpu_usage'] = np.random.uniform(20, 40)
            df.loc[i, 'memory_usage'] = np.random.uniform(95, 100)

        elif scenario == 'crash':  # Sistem Ã‡Ã¶kmesi (Ani dÃ¼ÅŸÃ¼ÅŸ)
            df.loc[i, 'cpu_usage'] = 0.0
            df.loc[i, 'memory_usage'] = 0.0

    print(f"âœ… {n_samples} satÄ±rlÄ±k veri seti '{DATA_FILE}' olarak kaydedildi.")
    df.to_csv(DATA_FILE, index=False)
    return df


def train_model():
    # 1. Veriyi YÃ¼kle
    if os.path.exists(DATA_FILE):
        df = pd.read_csv(DATA_FILE)
    else:
        df = generate_realistic_data()

    print("ðŸ§  Model eÄŸitimi baÅŸlÄ±yor...")

    # 2. Ã–lÃ§eklendirme (Scaling) - Ã‡OK Ã–NEMLÄ°
    # CPU 100, RAM 16000 olabilir. BunlarÄ± aynÄ± dÃ¼zleme (0-1 arasÄ± veya standart sapma) getirmeliyiz.
    scaler = StandardScaler()
    X_train = scaler.fit_transform(df[['cpu_usage', 'memory_usage']])

    # 3. Isolation Forest EÄŸitimi
    # contamination=0.02 -> Verinin %2'sinin kirli (anomali) olduÄŸunu biliyoruz.
    model = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)
    model.fit(X_train)

    # 4. Kaydetme (Hem Modeli Hem Scaler'Ä± kaydetmeliyiz!)
    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)

    joblib.dump(model, MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)  # Scaler'Ä± unutursak gelen veriyi normalize edemeyiz!

    print(f"ðŸŽ‰ BaÅŸarÄ±lÄ±! Model: {MODEL_PATH}, Scaler: {SCALER_PATH}")
    print("Åžimdi backend servisini yeniden baÅŸlat.")


if __name__ == "__main__":
    train_model()